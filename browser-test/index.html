<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live Translation Test</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .container {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .card {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .form-group {
            margin-bottom: 15px;
        }
        label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }
        select, textarea, input {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 16px;
        }
        textarea {
            min-height: 100px;
            resize: vertical;
        }
        button {
            background-color: #4CAF50;
            color: white;
            border: none;
            padding: 10px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
        }
        button:hover {
            background-color: #45a049;
        }
        button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        .result {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 4px;
            min-height: 100px;
        }
        .mic-button {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background-color: #f44336;
            margin: 20px auto;
        }
        .mic-button.recording {
            animation: pulse 1.5s infinite;
        }
        .mic-icon {
            width: 30px;
            height: 30px;
            fill: white;
        }
        .status {
            text-align: center;
            font-style: italic;
            color: #666;
        }
        .sample-phrases {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 10px;
        }
        .sample-phrase {
            background-color: #e9f7fe;
            border: 1px solid #b3e5fc;
            border-radius: 4px;
            padding: 5px 10px;
            cursor: pointer;
            font-size: 14px;
        }
        .sample-phrase:hover {
            background-color: #b3e5fc;
        }
        @keyframes pulse {
            0% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(244, 67, 54, 0.7);
            }
            70% {
                transform: scale(1.1);
                box-shadow: 0 0 0 10px rgba(244, 67, 54, 0);
            }
            100% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(244, 67, 54, 0);
            }
        }
    </style>
</head>
<body>
    <h1>Live Translation Test</h1>

    <div class="container">
        <!-- Text Translation Section -->
        <div class="card">
            <h2>Text Translation</h2>
            <div class="form-group">
                <label for="sourceLanguage">Source Language:</label>
                <select id="sourceLanguage">
                    <option value="auto">Auto-detect</option>
                    <option value="en">English</option>
                    <option value="es">Spanish</option>
                    <option value="fr">French</option>
                    <option value="de">German</option>
                    <option value="it">Italian</option>
                    <option value="ja">Japanese</option>
                    <option value="ko">Korean</option>
                    <option value="pt">Portuguese</option>
                    <option value="ru">Russian</option>
                    <option value="zh">Chinese</option>
                    <!-- More languages will be populated by JavaScript -->
                </select>
            </div>

            <div class="form-group">
                <label for="targetLanguage">Target Language:</label>
                <select id="targetLanguage">
                    <option value="en">English</option>
                    <option value="es">Spanish</option>
                    <option value="fr">French</option>
                    <option value="de">German</option>
                    <option value="it">Italian</option>
                    <option value="ja">Japanese</option>
                    <option value="ko">Korean</option>
                    <option value="pt">Portuguese</option>
                    <option value="ru">Russian</option>
                    <option value="zh">Chinese</option>
                    <!-- More languages will be populated by JavaScript -->
                </select>
            </div>

            <div class="form-group">
                <label for="inputText">Text to Translate:</label>
                <textarea id="inputText" placeholder="Enter text to translate..."></textarea>

                <div class="sample-phrases">
                    <div class="sample-phrase" data-lang="en" data-text="Hello, how are you today?">Hello (English)</div>
                    <div class="sample-phrase" data-lang="es" data-text="Hola, ¿cómo estás hoy?">Hola (Spanish)</div>
                    <div class="sample-phrase" data-lang="fr" data-text="Bonjour, comment allez-vous aujourd'hui?">Bonjour (French)</div>
                    <div class="sample-phrase" data-lang="de" data-text="Hallo, wie geht es Ihnen heute?">Hallo (German)</div>
                    <div class="sample-phrase" data-lang="ja" data-text="こんにちは、今日はお元気ですか？">こんにちは (Japanese)</div>
                </div>
            </div>

            <button id="translateButton">Translate</button>

            <div class="form-group">
                <label for="translationResult">Translation:</label>
                <div id="translationResult" class="result"></div>
            </div>
        </div>

        <!-- Voice Translation Section -->
        <div class="card">
            <h2>Voice Translation</h2>

            <div class="form-group">
                <label for="audioSource">Microphone:</label>
                <select id="audioSource">
                    <option value="">Default Microphone</option>
                    <!-- Audio sources will be populated by JavaScript -->
                </select>
            </div>

            <div class="form-group">
                <label for="voiceTargetLanguage">Target Language:</label>
                <select id="voiceTargetLanguage">
                    <option value="en">English</option>
                    <option value="es">Spanish</option>
                    <option value="fr">French</option>
                    <option value="de">German</option>
                    <option value="it">Italian</option>
                    <option value="ja">Japanese</option>
                    <option value="ko">Korean</option>
                    <option value="pt">Portuguese</option>
                    <option value="ru">Russian</option>
                    <option value="zh">Chinese</option>
                    <!-- More languages will be populated by JavaScript -->
                </select>
            </div>

            <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 10px;">
                <div>
                    <div class="mic-button" id="manualRecordButton" style="background-color: #2196F3;">
                        <svg class="mic-icon" viewBox="0 0 24 24">
                            <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                            <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                        </svg>
                    </div>
                    <p style="text-align: center; font-size: 12px;">Manual Recording</p>
                </div>

                <div>
                    <div class="mic-button" id="webrtcButton" style="background-color: #9C27B0;">
                        <svg class="mic-icon" viewBox="0 0 24 24">
                            <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                            <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                        </svg>
                    </div>
                    <p style="text-align: center; font-size: 12px;">WebRTC with VAD</p>
                </div>
            </div>

            <p class="status" id="micStatus">Click a microphone button to start recording</p>

            <div class="form-group" id="recordingControls" style="display: none; text-align: center;">
                <button id="stopRecordingButton" style="background-color: #f44336;">Stop Recording</button>
                <button id="submitRecordingButton" style="background-color: #4CAF50; margin-left: 10px;">Submit Recording</button>
            </div>

            <div style="margin-top: 10px; padding: 10px; background-color: #f8f9fa; border-radius: 4px; font-size: 12px;">
                <p style="margin: 0; font-style: italic;">
                    <strong>Note:</strong> This demo uses a custom implementation of Voice Activity Detection with WebRTC.
                    In a production environment, you would use Gemini Live API's built-in VAD capabilities through a server-side implementation.
                </p>
            </div>

            <div class="form-group">
                <label for="speechText">Recognized Speech:</label>
                <div id="speechText" class="result"></div>
            </div>

            <div class="form-group">
                <label for="voiceTranslationResult">Translation:</label>
                <div id="voiceTranslationResult" class="result"></div>
            </div>
        </div>
    </div>

    <script>
        // Configuration
        const API_BASE_URL = 'http://localhost:3001'; // Update with your server URL
        const WS_URL = 'ws://localhost:3001'; // WebSocket URL

        // WebSocket variables
        let socket = null;
        let socketSessionId = null;
        let isSocketConnected = false;
        let isRecording = false;
        let isManualRecording = false;
        let isWebRTCRecording = false;
        let mediaRecorder = null;
        let audioChunks = [];
        let audioBlob = null;
        let recognizedSpeech = '';
        let languages = [];

        // WebRTC and VAD variables
        let webrtcStream = null;
        let audioContext = null;
        let analyser = null;
        let vadProcessor = null;
        let isSpeaking = false;
        let silenceTimer = null;
        let speechTimer = null;
        let webrtcRecorder = null;
        let webrtcChunks = [];

        // DOM Elements
        const translateButton = document.getElementById('translateButton');
        const inputText = document.getElementById('inputText');
        const sourceLanguage = document.getElementById('sourceLanguage');
        const targetLanguage = document.getElementById('targetLanguage');
        const translationResult = document.getElementById('translationResult');
        const manualRecordButton = document.getElementById('manualRecordButton');
        const webrtcButton = document.getElementById('webrtcButton');
        const stopRecordingButton = document.getElementById('stopRecordingButton');
        const submitRecordingButton = document.getElementById('submitRecordingButton');
        const recordingControls = document.getElementById('recordingControls');
        const micStatus = document.getElementById('micStatus');
        const speechText = document.getElementById('speechText');
        const voiceTranslationResult = document.getElementById('voiceTranslationResult');
        const voiceTargetLanguage = document.getElementById('voiceTargetLanguage');
        const audioSource = document.getElementById('audioSource');
        const samplePhrases = document.querySelectorAll('.sample-phrase');

        // Fetch supported languages
        async function fetchLanguages() {
            try {
                const response = await fetch(`${API_BASE_URL}/languages`);
                const data = await response.json();
                languages = data.languages;
                populateLanguageDropdowns();
            } catch (error) {
                console.error('Error fetching languages:', error);
                alert('Failed to fetch supported languages. Please make sure the server is running.');
            }
        }

        // Populate language dropdowns
        function populateLanguageDropdowns() {
            const dropdowns = [sourceLanguage, targetLanguage, voiceTargetLanguage];

            // Clear existing options except the first one in sourceLanguage
            dropdowns.forEach(dropdown => {
                while (dropdown.options.length > (dropdown === sourceLanguage ? 1 : 0)) {
                    dropdown.remove(dropdown === sourceLanguage ? 1 : 0);
                }
            });

            // Add language options
            const languageNames = {
                'ar': 'Arabic',
                'bn': 'Bengali',
                'de': 'German',
                'en': 'English',
                'es': 'Spanish',
                'fr': 'French',
                'gu': 'Gujarati',
                'hi': 'Hindi',
                'id': 'Indonesian',
                'it': 'Italian',
                'ja': 'Japanese',
                'kn': 'Kannada',
                'ko': 'Korean',
                'ml': 'Malayalam',
                'mr': 'Marathi',
                'nl': 'Dutch',
                'pl': 'Polish',
                'pt': 'Portuguese',
                'ru': 'Russian',
                'ta': 'Tamil',
                'te': 'Telugu',
                'th': 'Thai',
                'tr': 'Turkish',
                'vi': 'Vietnamese',
                'cmn': 'Mandarin Chinese'
            };

            languages.forEach(lang => {
                const name = languageNames[lang] || lang;

                dropdowns.forEach(dropdown => {
                    if (dropdown === sourceLanguage && lang === 'auto') return; // Skip 'auto' for source as it's already added

                    const option = document.createElement('option');
                    option.value = lang;
                    option.textContent = name;
                    dropdown.appendChild(option);
                });
            });
        }

        // Text Translation
        async function translateText() {
            const text = inputText.value.trim();
            if (!text) {
                alert('Please enter text to translate');
                return;
            }

            const source = sourceLanguage.value;
            const target = targetLanguage.value;

            translateButton.disabled = true;
            translationResult.textContent = 'Translating...';

            try {
                const response = await fetch(`${API_BASE_URL}/translate`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        text,
                        sourceLang: source,
                        targetLang: target
                    })
                });

                const data = await response.json();

                if (data.error) {
                    translationResult.textContent = `Error: ${data.error}`;
                } else {
                    translationResult.textContent = data.translated;
                }
            } catch (error) {
                console.error('Translation error:', error);
                translationResult.textContent = 'Translation failed. Please try again.';
            } finally {
                translateButton.disabled = false;
            }
        }

        // Listen for changes to the audio source dropdown
        async function setupAudioSourceListener() {
            try {
                // Request microphone permission first
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

                // Stop the stream immediately - we just needed it to request permission
                stream.getTracks().forEach(track => track.stop());

                // Listen for changes to the audio source dropdown
                audioSource.addEventListener('change', () => {
                    const selectedDeviceId = audioSource.value;
                    console.log('Selected microphone changed:', selectedDeviceId);
                });

                // Update status to show microphone is available
                micStatus.textContent = 'Microphone access granted. Click a microphone button to start recording.';
            } catch (error) {
                console.error('Microphone access error:', error);
                micStatus.textContent = 'Failed to access microphone. Please ensure microphone permissions are granted.';
            }
        }

        // Sample phrases
        samplePhrases.forEach(phrase => {
            phrase.addEventListener('click', () => {
                const text = phrase.getAttribute('data-text');
                const lang = phrase.getAttribute('data-lang');

                inputText.value = text;

                // Set source language if it's not auto
                if (lang) {
                    for (let i = 0; i < sourceLanguage.options.length; i++) {
                        if (sourceLanguage.options[i].value === lang) {
                            sourceLanguage.selectedIndex = i;
                            break;
                        }
                    }
                }
            });
        });

        // Event Listeners
        translateButton.addEventListener('click', translateText);

        // Get available audio devices
        async function getAudioDevices() {
            try {
                // First request permission to access media devices
                await navigator.mediaDevices.getUserMedia({ audio: true });

                // Get list of media devices
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioInputs = devices.filter(device => device.kind === 'audioinput');

                // Clear existing options
                while (audioSource.options.length > 1) {
                    audioSource.remove(1);
                }

                // Add audio inputs to select element
                audioInputs.forEach(device => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Microphone ${audioSource.options.length}`;
                    audioSource.appendChild(option);
                });

                console.log('Audio devices loaded:', audioInputs.length);
            } catch (error) {
                console.error('Error getting audio devices:', error);
                micStatus.textContent = 'Error accessing microphones. Please check permissions.';
            }
        }

        // Manual recording with MediaRecorder
        async function setupManualRecording() {
            // Listen for manual recording button click
            manualRecordButton.addEventListener('click', startManualRecording);
            stopRecordingButton.addEventListener('click', stopManualRecording);
            submitRecordingButton.addEventListener('click', submitRecording);
        }

        async function startManualRecording() {
            if (isRecording || isManualRecording) return;

            try {
                // Get audio stream from selected device
                const deviceId = audioSource.value;
                const constraints = {
                    audio: deviceId ? { deviceId: { exact: deviceId } } : true
                };

                const stream = await navigator.mediaDevices.getUserMedia(constraints);

                // Reset recording state
                audioChunks = [];
                audioBlob = null;
                speechText.textContent = '';
                voiceTranslationResult.textContent = '';

                // Create media recorder
                mediaRecorder = new MediaRecorder(stream);

                // Listen for data available event
                mediaRecorder.addEventListener('dataavailable', event => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                });

                // Start recording
                mediaRecorder.start();
                isManualRecording = true;

                // Update UI
                micStatus.textContent = 'Recording... (Click Stop when finished)';
                manualRecordButton.style.backgroundColor = '#f44336';
                recordingControls.style.display = 'block';

                console.log('Manual recording started');
            } catch (error) {
                console.error('Error starting manual recording:', error);
                micStatus.textContent = 'Failed to start recording. Please check microphone permissions.';
            }
        }

        function stopManualRecording() {
            if (!isManualRecording || !mediaRecorder) return;

            try {
                // Stop recording
                mediaRecorder.stop();

                // Stop all tracks in the stream
                mediaRecorder.stream.getTracks().forEach(track => track.stop());

                // Update UI
                micStatus.textContent = 'Recording stopped. Click Submit to process or record again.';
                manualRecordButton.style.backgroundColor = '#2196F3';

                console.log('Manual recording stopped');
            } catch (error) {
                console.error('Error stopping manual recording:', error);
            }
        }

        async function submitRecording() {
            if (audioChunks.length === 0) {
                micStatus.textContent = 'No recording to submit. Please record audio first.';
                return;
            }

            try {
                // Create blob from audio chunks
                audioBlob = new Blob(audioChunks, { type: 'audio/webm' });

                // Create a temporary audio element to play back the recording
                const audioElement = document.createElement('audio');
                audioElement.controls = true;
                audioElement.src = URL.createObjectURL(audioBlob);

                // Add the audio element to the page
                speechText.innerHTML = '';
                speechText.appendChild(audioElement);

                // Add information about the audio format
                const formatInfo = document.createElement('div');
                formatInfo.style.fontSize = '12px';
                formatInfo.style.color = '#666';
                formatInfo.textContent = 'Audio format: WebM audio';
                speechText.appendChild(formatInfo);

                micStatus.textContent = 'Processing audio...';

                // Convert audio to text (simulated for this demo)
                const transcript = await convertAudioToText(audioBlob);

                if (transcript) {
                    // Add the transcript
                    const transcriptElement = document.createElement('p');
                    transcriptElement.innerHTML = `<strong>Transcript:</strong> ${transcript}`;
                    speechText.appendChild(transcriptElement);

                    // Now translate the transcript using our translation API
                    try {
                        const response = await fetch(`${API_BASE_URL}/translate`, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json'
                            },
                            body: JSON.stringify({
                                text: transcript,
                                targetLang: voiceTargetLanguage.value
                            })
                        });

                        const data = await response.json();

                        if (data.error) {
                            voiceTranslationResult.textContent = `Error: ${data.error}`;
                        } else {
                            voiceTranslationResult.textContent = data.translated;
                        }
                    } catch (error) {
                        console.error('Translation error:', error);
                        voiceTranslationResult.textContent = 'Translation failed. Please try again.';
                    }
                } else {
                    // If speech recognition failed, show a message
                    const messageElement = document.createElement('p');
                    messageElement.textContent = 'Could not transcribe audio. Please try speaking more clearly or use the text input instead.';
                    speechText.appendChild(messageElement);
                }

                // Reset recording state
                isManualRecording = false;
                recordingControls.style.display = 'none';

                micStatus.textContent = 'Audio processed. Click a microphone button to record again.';
            } catch (error) {
                console.error('Error submitting recording:', error);
                micStatus.textContent = 'Error processing audio.';
            }
        }

        // WebRTC with Voice Activity Detection
        async function setupWebRTCWithVAD() {
            // Listen for WebRTC button click
            webrtcButton.addEventListener('click', toggleWebRTCRecording);
        }

        async function toggleWebRTCRecording() {
            if (isWebRTCRecording) {
                stopWebRTCRecording();
            } else {
                startWebRTCRecording();
            }
        }

        async function startWebRTCRecording() {
            if (isRecording || isManualRecording || isWebRTCRecording) return;

            try {
                // Reset state
                webrtcChunks = [];
                speechText.textContent = '';
                voiceTranslationResult.textContent = '';

                // Get selected device
                const deviceId = audioSource.value;
                const constraints = {
                    audio: deviceId
                        ? {
                            deviceId: { exact: deviceId },
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 16000, // Gemini Live API expects 16kHz audio
                            sampleSize: 16,    // 16-bit PCM
                            channelCount: 1    // Mono audio
                        }
                        : {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 16000, // Gemini Live API expects 16kHz audio
                            sampleSize: 16,    // 16-bit PCM
                            channelCount: 1    // Mono audio
                        }
                };

                // Get audio stream
                webrtcStream = await navigator.mediaDevices.getUserMedia(constraints);

                // Create audio context with the correct sample rate for Gemini Live API
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 16000 // Ensure 16kHz sample rate for Gemini Live API
                });

                // Create source from stream
                const source = audioContext.createMediaStreamSource(webrtcStream);

                // Check if we need to resample
                if (audioContext.sampleRate !== 16000) {
                    console.log(`Resampling from ${audioContext.sampleRate}Hz to 16000Hz`);
                    micStatus.textContent = 'Resampling audio to 16kHz for optimal processing...';
                }

                // Create analyser
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048;
                analyser.smoothingTimeConstant = 0.8;
                source.connect(analyser);

                // Create script processor for VAD
                const bufferSize = 4096;
                vadProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);

                // Connect processor
                analyser.connect(vadProcessor);
                vadProcessor.connect(audioContext.destination);

                // Setup VAD
                vadProcessor.onaudioprocess = detectSpeech;

                // Create media recorder for the stream
                webrtcRecorder = new MediaRecorder(webrtcStream);

                webrtcRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        webrtcChunks.push(event.data);

                        // Send audio chunk to WebSocket if connected and speaking
                        if (isSocketConnected && isSpeaking) {
                            sendAudioToWebSocket(event.data);
                        }
                    }
                };

                webrtcRecorder.onstop = processWebRTCAudio;

                // Start recording
                webrtcRecorder.start();
                isWebRTCRecording = true;

                // Update UI
                webrtcButton.style.backgroundColor = '#f44336';
                micStatus.textContent = 'WebRTC recording active. Waiting for speech...';

                console.log('WebRTC recording started with VAD');
            } catch (error) {
                console.error('Error starting WebRTC recording:', error);
                micStatus.textContent = 'Failed to start WebRTC recording. Please check permissions.';
            }
        }

        function detectSpeech(event) {
            if (!isWebRTCRecording) return;

            // Get audio data
            const buffer = new Uint8Array(analyser.frequencyBinCount);
            analyser.getByteFrequencyData(buffer);

            // Calculate average volume
            let sum = 0;
            for (let i = 0; i < buffer.length; i++) {
                sum += buffer[i];
            }
            const average = sum / buffer.length;

            // Threshold for speech detection - using a dynamic threshold system
            // This is similar to how Gemini's VAD works, adapting to background noise
            const threshold = calculateDynamicThreshold(average);

            // Detect speech
            if (average > threshold) {
                if (!isSpeaking) {
                    onSpeechStart();
                }

                // Reset silence timer if it's running
                if (silenceTimer) {
                    clearTimeout(silenceTimer);
                    silenceTimer = null;
                }

                // Update speech timer
                if (!speechTimer) {
                    speechTimer = setTimeout(() => {
                        // If speaking continues for a while, update status
                        micStatus.textContent = 'Speech detected! Recording...';
                    }, 500);
                }
            } else if (isSpeaking) {
                // Reset speech timer if it's running
                if (speechTimer) {
                    clearTimeout(speechTimer);
                    speechTimer = null;
                }

                // Start silence timer if not already running
                if (!silenceTimer) {
                    silenceTimer = setTimeout(() => {
                        onSpeechEnd();
                    }, 1500); // 1.5 seconds of silence to end speech segment
                }
            }
        }

        // Recent audio levels for dynamic threshold calculation
        const recentLevels = [];
        const MAX_RECENT_LEVELS = 100;

        // Calculate a dynamic threshold based on recent audio levels
        // This mimics how Gemini's VAD adapts to background noise
        function calculateDynamicThreshold(currentLevel) {
            // Add current level to recent levels
            recentLevels.push(currentLevel);

            // Keep only the most recent levels
            if (recentLevels.length > MAX_RECENT_LEVELS) {
                recentLevels.shift();
            }

            // Calculate baseline (background noise level)
            // Using the 15th percentile as an estimate of background noise
            const sortedLevels = [...recentLevels].sort((a, b) => a - b);
            const baselineIndex = Math.floor(sortedLevels.length * 0.15);
            const baseline = sortedLevels[baselineIndex] || 0;

            // Set threshold above the baseline
            // The multiplier determines sensitivity - higher = less sensitive
            const thresholdMultiplier = 1.8;
            const minThreshold = 10; // Minimum threshold to avoid false positives

            return Math.max(baseline * thresholdMultiplier, minThreshold);
        }

        function onSpeechStart() {
            console.log('Speech started');
            isSpeaking = true;
            micStatus.textContent = 'Speech detected! Recording...';

            // Start WebSocket transcription if connected
            if (isSocketConnected) {
                startWebSocketTranscription();
            }
        }

        function onSpeechEnd() {
            console.log('Speech ended');
            isSpeaking = false;

            // Stop WebSocket transcription if connected
            if (isSocketConnected) {
                stopWebSocketTranscription();
            }

            // Stop the current recording and process it
            if (webrtcRecorder && webrtcRecorder.state === 'recording') {
                webrtcRecorder.stop();

                // Start a new recording session
                setTimeout(() => {
                    if (isWebRTCRecording) {
                        webrtcRecorder = new MediaRecorder(webrtcStream);
                        webrtcRecorder.ondataavailable = (event) => {
                            if (event.data.size > 0) {
                                webrtcChunks.push(event.data);

                                // Send audio chunk to WebSocket if connected
                                if (isSocketConnected && isSpeaking) {
                                    sendAudioToWebSocket(event.data);
                                }
                            }
                        };
                        webrtcRecorder.onstop = processWebRTCAudio;
                        webrtcRecorder.start();
                        micStatus.textContent = 'Processing speech... Waiting for more speech.';
                    }
                }, 100);
            }
        }

        async function processWebRTCAudio() {
            if (webrtcChunks.length === 0) return;

            try {
                // Create blob from chunks - use the correct MIME type for our transcription API
                const audioBlob = new Blob(webrtcChunks, { type: 'audio/webm' });

                // Clear chunks for next recording
                webrtcChunks = [];

                // Create a temporary audio element to play back the recording
                const audioElement = document.createElement('audio');
                audioElement.controls = true;
                audioElement.src = URL.createObjectURL(audioBlob);

                // Add the audio element to the page
                speechText.innerHTML = '';
                speechText.appendChild(audioElement);

                // Add information about the audio format
                const formatInfo = document.createElement('div');
                formatInfo.style.fontSize = '12px';
                formatInfo.style.color = '#666';

                // Safely access audioContext if it exists
                const sampleRate = audioContext ? audioContext.sampleRate : 16000;
                formatInfo.textContent = `Audio format: 16-bit PCM at ${sampleRate}Hz`;
                speechText.appendChild(formatInfo);

                // Convert audio to text using our transcription API
                const transcript = await convertAudioToText(audioBlob);

                if (transcript) {
                    // Add the transcript
                    const transcriptElement = document.createElement('p');
                    transcriptElement.innerHTML = `<strong>Transcript:</strong> ${transcript}`;
                    speechText.appendChild(transcriptElement);

                    // Now translate the transcript using our translation API
                    try {
                        const response = await fetch(`${API_BASE_URL}/translate`, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json'
                            },
                            body: JSON.stringify({
                                text: transcript,
                                targetLang: voiceTargetLanguage.value
                            })
                        });

                        const data = await response.json();

                        if (data.error) {
                            voiceTranslationResult.textContent = `Error: ${data.error}`;
                        } else {
                            voiceTranslationResult.textContent = data.translated;
                        }
                    } catch (error) {
                        console.error('Translation error:', error);
                        voiceTranslationResult.textContent = 'Translation failed. Please try again.';
                    }
                } else {
                    // If speech recognition failed, show a message
                    const messageElement = document.createElement('p');
                    messageElement.textContent = 'Could not transcribe audio. Please try speaking more clearly or use the text input instead.';
                    speechText.appendChild(messageElement);
                }
            } catch (error) {
                console.error('Error processing WebRTC audio:', error);
                speechText.textContent = 'Error processing audio.';
            }
        }

        // Function to convert audio blob to text using our transcription API
        async function convertAudioToText(audioBlob) {
            try {
                // Show a loading message
                const loadingElement = document.createElement('p');
                loadingElement.textContent = 'Transcribing audio...';
                speechText.appendChild(loadingElement);

                // Send the audio blob to our transcription API
                const response = await fetch(`${API_BASE_URL}/transcribe`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': audioBlob.type
                    },
                    body: audioBlob
                });

                // Check if the request was successful
                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(errorData.error || 'Failed to transcribe audio');
                }

                // Parse the response
                const data = await response.json();

                // Remove the loading message
                speechText.removeChild(loadingElement);

                // Return the transcription
                return data.transcription;
            } catch (error) {
                console.error('Transcription error:', error);
                return null;
            }
        }

        function stopWebRTCRecording() {
            if (!isWebRTCRecording) return;

            try {
                // Stop recording
                if (webrtcRecorder && webrtcRecorder.state === 'recording') {
                    webrtcRecorder.stop();
                }

                // Stop VAD processor
                if (vadProcessor) {
                    vadProcessor.disconnect();
                    vadProcessor = null;
                }

                // Stop audio context
                if (audioContext) {
                    audioContext.close();
                    audioContext = null;
                }

                // Stop all tracks in the stream
                if (webrtcStream) {
                    webrtcStream.getTracks().forEach(track => track.stop());
                    webrtcStream = null;
                }

                // Clear timers
                if (silenceTimer) {
                    clearTimeout(silenceTimer);
                    silenceTimer = null;
                }

                if (speechTimer) {
                    clearTimeout(speechTimer);
                    speechTimer = null;
                }

                // Update state
                isWebRTCRecording = false;
                isSpeaking = false;

                // Reset the recent levels array for VAD
                if (typeof recentLevels !== 'undefined' && Array.isArray(recentLevels)) {
                    recentLevels.length = 0;
                }

                // Update UI
                webrtcButton.style.backgroundColor = '#9C27B0';
                micStatus.textContent = 'WebRTC recording stopped.';

                console.log('WebRTC recording stopped');
            } catch (error) {
                console.error('Error stopping WebRTC recording:', error);
            }
        }

        // WebSocket Functions
        function setupWebSocket() {
            // Close any existing connection
            if (socket) {
                socket.close();
                socket = null;
                socketSessionId = null;
                isSocketConnected = false;
            }

            // Create new WebSocket connection
            socket = new WebSocket(WS_URL);

            // Connection opened
            socket.addEventListener('open', (event) => {
                console.log('WebSocket connection established');
                isSocketConnected = true;
                micStatus.textContent = 'WebSocket connected. Ready for real-time transcription.';
            });

            // Listen for messages
            socket.addEventListener('message', (event) => {
                try {
                    const message = JSON.parse(event.data);
                    handleWebSocketMessage(message);
                } catch (error) {
                    console.error('Error parsing WebSocket message:', error);
                }
            });

            // Connection closed
            socket.addEventListener('close', (event) => {
                console.log('WebSocket connection closed');
                isSocketConnected = false;
                socketSessionId = null;

                // Try to reconnect after a delay
                setTimeout(() => {
                    if (!isSocketConnected) {
                        setupWebSocket();
                    }
                }, 3000);
            });

            // Connection error
            socket.addEventListener('error', (event) => {
                console.error('WebSocket error:', event);
                isSocketConnected = false;
            });
        }

        // Handle WebSocket messages
        function handleWebSocketMessage(message) {
            console.log('Received WebSocket message:', message);

            switch (message.type) {
                case 'session':
                    // Store session ID
                    socketSessionId = message.sessionId;
                    console.log('WebSocket session established:', socketSessionId);
                    break;

                case 'transcription':
                    // Display transcription
                    if (message.text) {
                        // Add the transcript to the page
                        const transcriptElement = document.createElement('p');
                        transcriptElement.innerHTML = `<strong>Transcript:</strong> ${message.text}`;
                        speechText.appendChild(transcriptElement);
                    }
                    break;

                case 'translation':
                    // Display translation
                    if (message.text) {
                        voiceTranslationResult.textContent = message.text;
                    }
                    break;

                case 'error':
                    // Display error
                    console.error('WebSocket error:', message.error);
                    micStatus.textContent = `Error: ${message.error}`;
                    break;

                default:
                    console.log('Unknown message type:', message.type);
            }
        }

        // Send audio data to WebSocket
        function sendAudioToWebSocket(audioBlob) {
            if (!isSocketConnected || !socket || socket.readyState !== WebSocket.OPEN) {
                console.error('WebSocket not connected');
                return false;
            }

            // Send audio data
            socket.send(audioBlob);
            return true;
        }

        // Start WebSocket transcription session
        function startWebSocketTranscription() {
            if (!isSocketConnected || !socket || socket.readyState !== WebSocket.OPEN) {
                console.error('WebSocket not connected');
                return false;
            }

            // Send start command
            socket.send(JSON.stringify({
                type: 'start',
                format: 'audio/webm'
            }));

            // Set target language
            socket.send(JSON.stringify({
                type: 'setLanguage',
                language: voiceTargetLanguage.value
            }));

            return true;
        }

        // Stop WebSocket transcription session
        function stopWebSocketTranscription() {
            if (!isSocketConnected || !socket || socket.readyState !== WebSocket.OPEN) {
                console.error('WebSocket not connected');
                return false;
            }

            // Send stop command
            socket.send(JSON.stringify({
                type: 'stop'
            }));

            return true;
        }

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            fetchLanguages();
            getAudioDevices();
            setupAudioSourceListener();
            setupManualRecording();
            setupWebRTCWithVAD();
            setupWebSocket();
        });
    </script>
</body>
</html>
